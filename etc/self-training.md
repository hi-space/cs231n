# Self-Supervised Training

높은 확률값이 나오면 가중치를 주는 간단한 방식으로 손쉽게 모델의 성능 향상을 꾀할 수 있는 테크닉입니다.

 셀프 트레이닝의 원리는 매우 간단합니다. 바로 **높은 확률값이 나오는 데이터** 위주로 가져가겠다는 겁니다. 예를들어 로지스틱 회귀분석\(logistic regression\) 결과 한 데이터에 대한 1일 확률값이 0.95가 나온 경우가 있고 0.55가 나온 경우가 있다면, 0.95로 예측한 데이터를 가져간다는 뜻입니다.

### Procedure <a id="procedure"></a>

간단한 셀프 트레이닝의 알고리즘은 다음과 같습니다.

* 레이블이 달린 데이터로 모델을 학습시킵니다.
* 이 모델을 가지고 레이블이 달리지 않은 데이터를 예측합니다.
* 이중에서 가장 확률값이 높은 데이터들만 레이블 데이터로 다시 가져갑니다.
* 위 과정을 계속 반복합니다.

이런 방식으로 알고리즘을 반복하면 점점 더 모델이 정확해질 수 있다는게 셀프 트레이닝의 핵심 개념입니다. 과정을 그림으로 나타내면 다음과 같습니다.

[![](https://i.imgur.com/puPDnOX.png)](https://imgur.com/puPDnOX)



 k-nearest neighbor 모델에서 k에 1을 넣고도 셀프 트레이닝이 가능합니다. 먼저 레이블 데이터에서 **가장 가까운 1개의** 언레이블 데이터를 집습니다. 그리고 이 언레이블 데이터를 레이블 데이터와 같은 클래스로 설정을 합니다. 이 작업을 언레이블 데이터가 없어질 때까지 계속 반복하면 됩니다. 아래 그림에서 초록색이 언레이블 데이터, 빨간색과 파란색이 각 클래스별 레이블 데이터입니다. 이터레이션을 거칠수록 점점 제대로 분류를 하는 것을 볼 수 있습니다.

![](../.gitbook/assets/image%20%28250%29.png)

정리하자면 셀프 트레이닝의 장점은 다음과 같습니다.

* 가장 간단한 준지도학습\(semi-supervised learning\)이다.
* 어떤 알고리즘이라고 적용 가능하다\(wrapper method\).
* NLP 같은 분야에서 종종 쓰인다.

단점은 다음과 같습니다.

* 초반에 잘못 저지른 실수\(early mistakes\)가 잘못된 길로 인도할 수 있다.
* 완전히 수렴\(convergence\)한다고 딱 말할 수 없다\(Cannot say too much in terms of convergence\)

